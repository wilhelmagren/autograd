{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbdb706",
   "metadata": {},
   "source": [
    "# Introduction to computational graphs for deep learning\n",
    "This interactive notebook will present some of the basic concepts that state-of-the-art deep learning frameworks like PyTorch and Tensorflow are built upon. The reader is expected to have adequate theoretical and practical knowledge regarding multivariable calculus and probabilistic methods. Furthermore, after going through this notebook I am confident that the reader can implement a small-scale library themselfs, which implements the process of graph building and automatic calculation of the analytical gradient for a deep learning model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77eca54",
   "metadata": {},
   "source": [
    "### Background\n",
    "*Feedforward* artificial neural networks (ANN) can be regarded as a collection of perceptrons (nodes) that each belong to  a respective layer that depend on some previous input to yield a response. A standard one-layer ANN usually performs the mapping $y = h(x)$ where $h(x) = Wx + b$ and $W$ are the learnable weights of each respective node $w_{ij}$ and $b$ is a learnable bias. A multi-layer ANN (MLP) on the other hand performs a multitude of these mappings, and can thus be formalized as $y = h_n(h_{n-1}(...(h_1(x)))$. Because of the depth of layers dependent on each other MLPs require the *backpropagation* algorithm to calculate the gradients of the weights $W$. Furthermore, if there are no non-linear activations between the layers of an MLP, it can always be summarized as a single-layer ANN. Therefore, an *activation function* has to be applied after the outputs of a layer. Two great historical alternatives are the sigmoid function $\\sigma$ and the hypoerbolic tangent function $tanh$\n",
    "\n",
    ">$\\large \\sigma(x) = \\frac{1}{1 + e^{-x}}$<br>$\\large tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "\n",
    "In modern day frameworks the backpropagation algorithm is implemented by means of computational graphs. These are formally defined as directed acyclic graphs (DAG) that directly follow the process from an input $x$ to output $y$ mapped by the model $f$. The DAG is precisely what it sounds like, a directed graph with no cycles. If there were cycles, then the backpropagation algorithm would never terminate as there would always be a next layer to calculate the gradient based on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320153b",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "In a supervised learning context one can always measure some form of error or loss of the network after performing inference. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f175d2",
   "metadata": {},
   "source": [
    "### Constructing the DAG and performing the forward pass\n",
    "The *forward pass* specifies walking through the DAG in the true direction of the graph, practically meaning, from input to output. Let's start with a simple example. We want to perform the affine transform $y = f(x)$ where $f(x) = Wx + b$. Visually, this can be represented by the DAG below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf5793",
   "metadata": {},
   "source": [
    "\n",
    "![DAG1.svg](../images/DAG1.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed1d9b5",
   "metadata": {},
   "source": [
    "Here we denote the result of the operation $Wx$ as $g$, since we need a way to represent all intermediate step. Lets now assume that we are in a supervised learning context and we know the true value to $y$ which is $y'$ and we want to measure the loss $L$ by taking their difference according to $L = y' - y$. Then the entire DAG from input $x$ to final loss $L$ with all intermediate steps can be represented as\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d763b0fc",
   "metadata": {},
   "source": [
    "![DAG2.svg](../images/DAG2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae76d56",
   "metadata": {},
   "source": [
    "### Performing the backward pass in the DAG \n",
    "Lets assume we got a large loss, indicating that our prediction was incorrect and thus we want to update our weight $W$ and bias $b$ of the model. The steps of the *backward pass* is in accordance with the backpropagation algorithm, where the the gradient of the loss $L$ is calculated based on the paramater and multiplied with the previous layers nodes output. The first step thus becomes to calculate the gradient of the loss, with respect to the loss\n",
    "\n",
    ">$\\large \\frac{\\partial L}{\\partial L} = 1$\n",
    "\n",
    "which ofcourse is always equal to 1. The second step is to find the gradient of the operation that gave us the loss $L = y' - y$, and this yields the gradients\n",
    "\n",
    ">$\\large \\frac{\\partial L}{\\partial y'} = 1$<br>$\\large \\frac{\\partial L}{\\partial y} = -1$\n",
    "\n",
    "continuing with the backwards pass we come to the operation $y = g + b$. Here we can utilize the chain-rule to calculate the gradient of $g$ and $b$ respectively, since we already know the gradient of the loss with respect to $y$,\n",
    "\n",
    ">$\\large \\frac{\\partial L}{\\partial g} = \\frac{\\partial L}{\\partial y}*\\frac{\\partial y}{\\partial g} = -1 * \\frac{\\partial y}{\\partial g} = -1 * 1$<br>$\\large \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y}*\\frac{\\partial y}{\\partial b} = -1*\\frac{\\partial y}{\\partial b} = -1 * 1$\n",
    "\n",
    "Already at this point we can see the beauty and simplicity of the DAG in a backpropagation scenario. Instead of working with all neurons of a layer directly, they are summarized as a parameter and their operation on the input is represented as a node in the DAG. It becomes intuitive to watch the gradient flowing backwards through the model, utilizing the chain-rule to analytically calculate the gradients of each node in a sequential manner. Anyway, lets continue with our backwards pass. Now that we know the gradient of the loss $L$ with respect to $g$, we can find the gradient of the weight parameter $W$ for our model. Similarly as in the previous steps, utilizing the chain-rule, this becomes\n",
    "\n",
    ">$\\large \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y}*\\frac{\\partial y}{\\partial g}*\\frac{\\partial g}{\\partial W} = -1 * 1 * \\frac{\\partial g}{\\partial W} = -1 * 1 * x$<br>$\\large \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y}*\\frac{\\partial y}{\\partial g}*\\frac{\\partial g}{\\partial x} = -1 * 1 * \\frac{\\partial g}{\\partial x} = -1 * 1 * W$\n",
    "\n",
    "and we have analytically found the two gradients that we require to update our model parameters $W$ and $b$ such that our next prediction would yield a smaller loss! Even though we calculate gradients of the loss with respect to all variables involved in the DAG it is not always the case that we want to, or even should, update them. For example, in this backward pass process we accumulate gradients for both $x$ and $y'$, but these are constant data that we are trying to map with our model parameters. Thus, we do **not** want to update their initial values based on their respective gradients. So when implementing the DAG and backward pass, pay close attention to what variables you are applying the gradient to...\n",
    "\n",
    "The DAG with respective gradients in green can be seen below (in case the math wasn't very informative...)\n",
    "\n",
    "![DAG3.svg](../images/DAG3.svg)\n",
    "\n",
    "This is pretty much all the theory you need to implement your own autograd library! Below is an extremely barebones implementation of a single-layer feedforward ANN that is trained to predict the values of a vector. This is performed by means of minizing the mean squared error and applying gradient updates with stochastic gradient descent (SGD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc473116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The tensor class we are building for the DAG construction holds a data attribute which are of type np.ndarray,\n",
    "apart from that, no other libraries are requires. Partialfunction is used to freeze the ops classes after being\n",
    "used on some data, such that the tensors of interest can be saved and used for analytical gradient computation.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partialmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c9b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    def __init__(self, data, requires_grad=True):\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "        self._ctx = None\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            data = np.array(data, dtype=np.float32)\n",
    "        elif isinstance(data, np.float32):\n",
    "            data = np.array(data, dtype=np.float32)\n",
    "        \n",
    "        self.data = data\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'<tensor.Tensor {self.data}, grad={self.grad}, _ctx={self._ctx}>'\n",
    "    \n",
    "    def backward(self, allow_fill=True):\n",
    "        if self._ctx is None:\n",
    "            return\n",
    "        \n",
    "        if self.grad is None and allow_fill:\n",
    "            self.grad = np.ones_like(self.data)\n",
    "        \n",
    "        gradients = self._ctx.backward(self._ctx, self.grad)\n",
    "        gradients = [gradients] if len(self._ctx.parents) == 1 else gradients\n",
    "        for tensor, gradient in zip(self._ctx.parents, gradients):\n",
    "            if gradient is None:\n",
    "                continue\n",
    "                \n",
    "            tensor.grad = gradient\n",
    "            tensor.backward(allow_fill=False)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.data.dtype\n",
    "    \n",
    "    @classmethod\n",
    "    def uniform(cls, *shape, **kwargs):\n",
    "        return cls(np.random.uniform(-1., 1., size=shape)\n",
    "                   / np.sqrt(np.prod(shape)).astype(np.float32), **kwargs)\n",
    "    \n",
    "    def mean(self):\n",
    "        div = Tensor(np.array([1 / self.data.size], dtype=self.data.dtype))\n",
    "        return self.sum().mul(div)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5fe18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function(object):\n",
    "    def __init__(self, *tensors):\n",
    "        self.parents = tensors\n",
    "        self.saved_tensors = []\n",
    "        self.requires_grad = any([tensor.requires_grad for tensor in tensors])\n",
    "    \n",
    "    def save_for_backward(self, *x):\n",
    "        self.saved_tensors.extend(x)\n",
    "    \n",
    "    def apply(self, arg, *x):\n",
    "        ctx = arg(self, *x)\n",
    "        output = Tensor(arg.forward(ctx, self.data, *[tensor.data for tensor in x]))\n",
    "        output._ctx = ctx\n",
    "        return output\n",
    "\n",
    "\n",
    "class Add(Function):\n",
    "    @staticmethod\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self, grad_out):\n",
    "        return grad_out, grad_out\n",
    "\n",
    "\n",
    "class Mul(Function):\n",
    "    @staticmethod\n",
    "    def forward(self, x, y):\n",
    "        self.save_for_backward(x, y)\n",
    "        return x * y\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self, grad_out):\n",
    "        x, y = self.saved_tensors\n",
    "        return y * grad_out, x * grad_out\n",
    "\n",
    "\n",
    "class Sub(Function):\n",
    "    @staticmethod\n",
    "    def forward(self, x, y):\n",
    "        return x - y\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self, grad_out):\n",
    "        return grad_out, -grad_out\n",
    "\n",
    "    \n",
    "class Sum(Function):\n",
    "    @staticmethod\n",
    "    def forward(self, x):\n",
    "        self.save_for_backward(x)\n",
    "        return np.array([x.sum()])\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self, grad_out):\n",
    "        x, = self.saved_tensors\n",
    "        return grad_out * np.ones_like(x)\n",
    "\n",
    "\n",
    "__allops__ = {\n",
    "    'add': Add,\n",
    "    'mul': Mul,\n",
    "    'sub': Sub,\n",
    "    'sum': Sum\n",
    "}\n",
    "\n",
    "\n",
    "def register_all():\n",
    "    for name, func in __allops__.items():\n",
    "        setattr(Tensor, name, partialmethod(func.apply, func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e7fe25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(98)\n",
    "register_all()\n",
    "\n",
    "history = []\n",
    "learning_rate = 0.1\n",
    "x = Tensor([.5, .1, -.7], requires_grad=False)\n",
    "W = Tensor.uniform(3)\n",
    "b = Tensor([1.])\n",
    "y_true = Tensor([-1., .7, -.5], requires_grad=False)\n",
    "\n",
    "# Perform a couple of forward pass specified by the DAG\n",
    "# and perform SGD optimizer update on W and b\n",
    "for _ in range(100):\n",
    "    # get output prediction y\n",
    "    g = x.mul(W)\n",
    "    y = g.add(b)\n",
    "    # calculate MSE\n",
    "    error = y_true.sub(y)\n",
    "    loss = error.mul(error).mean()\n",
    "    loss.backward()\n",
    "    history.append(loss.data)\n",
    "\n",
    "    W.data = W.data - learning_rate * W.grad\n",
    "    b.data = b.data - learning_rate * b.grad\n",
    "\n",
    "plt.plot(history) \n",
    "!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
