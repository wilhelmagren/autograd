{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffffd80a",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "Below is a detailed explanation of artificial neurons, perceptrons, and multi-layer perceptrons. The reader is expected to have some knowledge about machine learning prior to reading this, but also be not too familiar with neural networks. The reader should after this notebook see the one regarding *computational graphs in deep learning* to get a practical and theoretical understanding of deep learning frameworks. \n",
    "\n",
    "----------------------------------\n",
    "\n",
    "\n",
    "## Biological neurons\n",
    "\n",
    "The first computational model of an *artificial neuron* was proposed by McCulloch \\& Pitts way back in 1943. It still serves as the basis for almost all modern day artifical neural networks (ANN). The McCulloch \\& Pitts model took more of a biological approach to learning, and strived to mimick the functionality of a biological neuron. Extremely simplified, the biological neuron consists of four main components: \n",
    "- *dendrite*, branched extensions of a nerve cell, receives signals from other neurons in form of chemically induced potentials,\n",
    "- *soma*, the cell body processing the received information,\n",
    "- *axon*, nerver fiber which transmits the output of the soma,\n",
    "- *synapse* is the point of connection to other neurons.\n",
    "\n",
    "Generally, and somewhat biologically inaccurate, the process for the neurons in our brain is that the dendrites receive output from either another neuron or some stimuli, the soma processes it and transmits its calculated response to other neurons if it is *activated*. Ofcourse, this process is happening in billions of neurons in parallel, and potentially in a hierarchical systematic manner. This ensures that there is a divion of labor so to say, i.e. each neuron only activates when its inherent criteria is met, for example an amount of potential being received. A schematic image of a neuron can be seen below.\n",
    "![neuron_schematic](../images/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1228ff49",
   "metadata": {},
   "source": [
    "## Artificial neurons\n",
    "\n",
    "Reconnecting to the computational model proposed by McCulloch \\& Pitts, the biological neuron is simplified to the *artificial neuron*. Here, the dendrites are represented as input values to the soma, and the axon is the output of the processed input by the soma. In modern day this mathematical discriminator is referred to as a *perceptron*, and can be used in supervised learning contexts. The perceptron comprises two main components:\n",
    "- $g$, aggregates the input values which represent excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites,\n",
    "- $\\sigma$, activation function applied to the aggregated response $g$, based on value of $f$ either activates or not.\n",
    "\n",
    "Since the activation function $\\sigma$ of the perceptron either activates the neuron or not, it can be viewed as a binary classifier. Each input to the neuron has an associated weight, deciding how much each input should contribute to the activation of the neuron. This old-school perceptron has been modified in modern day machine learning paradigms with alternative activation functions $\\sigma$ that allow for continuous output values. Anyway, the perceptron can be formally defined as\n",
    "\n",
    ">$\\large g = \\sum_i x_i\\cdot w_i$\n",
    "\n",
    ">$\\large \\hat{y} = \\sigma(g)$\n",
    "\n",
    "where $x$ are the inputs, $w$ are the weights, and $\\hat{y}$ is the *predicted* activation response. I say predicted here, because in a supervised learning context we know what the response should be and can tweak the neurons weights if it activates incorrectly. This artifical neuron can conceptually be reconnected to its biological counterpart by the following image.\n",
    "![artificial neuron](../images/artificial_neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1423747",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Single-layer perceptrons\n",
    "Utilizing only a single perceptron is not as common today. A single perceptron is only capable of correctly performing classification on a *linearly-separable* dataset. What this means is that there are two classes in the dataset which distinctly can be separated by a single line. This might seem extremely limiting to you, and that would be correct. So how can we utilize the perceptron to work on more challenging tasks, like classifying on data that is not linearly separable like the *xor* operation? \n",
    "\n",
    "\n",
    "### Linearity\n",
    "Your first idea might be to introduce more perceptrons, and that is exactly what you would do. However, one has to do this with great care. Summarizing several perceptrons is only meaningful if you introduce non-linear activation functions. Otherwise, the contribution of all linear perceptrons can be summarized as a single perceptron. Lets visualize this with the below example (omitting the activation function $\\sigma$). \n",
    "\n",
    "![perceptrons](../images/perceptrons.svg)\n",
    "\n",
    "The above graph represent our three perceptrons that work on the input $x$ to produce the output $y$. Here I also introduce the bias term $b_i$ that is commonly used in perceptrons. Mathematically this can be written as\n",
    "\n",
    ">$\\large g = x_1\\cdot w_1 + x_2\\cdot w_2 + b_1$\n",
    "\n",
    ">$\\large f = x_3\\cdot w_3 + x_4\\cdot w_4 + b_2$\n",
    "\n",
    ">$\\large y = g + f + b_3$\n",
    "\n",
    "Everyone can see that it does not matter what $f$ or $g$ does individally, as all of the operations on the input data can directly be expressed at $y$ accordingly\n",
    "\n",
    ">$\\large y =  x_1\\cdot w_1 + \\dots + x_4\\cdot w_4 + b_1 + b_2 + b_3 $\n",
    "\n",
    "and if we were to introduce an activation function $\\sigma$ to the output $y$ we would get the resulting $\\hat{y}$ that is our predicted class or value\n",
    "\n",
    ">$\\large \\hat{y} = \\sigma(y) = \\sigma(x_1\\cdot w_1 + \\dots + x_4\\cdot w_4 + b_1 + b_2 + b_3)$\n",
    "\n",
    "What we see here is that we are fully capable of expressing the activation output *without* including the intermediate results $g$ and $f$. Thus, the above graph can be represented by a single perceptron. However, introduce non-linear activation functions for $f$ and $g$ and we have to include these in the final expression for the activation output. What I shoved above is an example of a single-layer perceptron. We have perceptrons interacting directly with the input and output of the model, and those two *layers* subsequentially interact with each other, but no other layers are included.\n",
    "\n",
    "### Updating the weights\n",
    "The perceptron learning rule dictates the fashion for how the dendritic weights should be updated. There are several different learning rules that can be applied, but all follow the same approach. First, propagate your input $x$ across the perceptron, acquiring the predicted activation output $\\hat{y}$. Given the real value to learn $y$, and a so-called *learning rate* $\\eta$, we can calculate the weight update accordingly\n",
    "\n",
    ">$\\large \\Delta w_i = -\\eta(y - \\hat{y})\\cdot x_i $\n",
    "\n",
    "Alternative learning methods usually build upon the concept of *gradient descent*, where the error expression $(y - \\hat{y})\\cdot x_i$ is substituted in favor for a loss function $\\mathcal{L}$. The weight update is then found by calculating the gradient of the loss w.r.t the weight\n",
    "\n",
    ">$\\large \\Delta w_i = -\\eta\\frac{\\partial \\mathcal{L}}{\\partial w_i}$\n",
    "\n",
    "There are multiple approaches to this gradient descent, often referred to as *optimizers* since they aim to optimize the weights of the perceptron. Some implementations like *SGD*, *Adam*, and *RMSProp* can be found implemented in the autograd library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "23456da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f159a4369d0>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)\n",
    "\n",
    "# we represent our input data as the AND logical gate, from Boolesk algebra\n",
    "# furthermore, we introduce the bias as 1 in each sample point X[], this makes\n",
    "# the linear algebra very easy to work with\n",
    "# and now since the constant bias term in the input data has been added in the\n",
    "# last position of the data sample, the corresponding weight bias is the third\n",
    "# item of the weight matrix.\n",
    "X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]]).astype(np.float32)\n",
    "W = np.random.uniform(-1.0, 1.0, size=(3, 1))\n",
    "y = np.array([0.0, 0.0, 0.0, 1.0]).reshape((-1, 1))\n",
    "\n",
    "# general perceptron learning formula:\n",
    "# error = targets - predictions\n",
    "# update_W = -learning_rate * error * X\n",
    "def forward(X, W, y, lr=1e-1):\n",
    "    preds = X @ W\n",
    "    preds[preds > 0.0] = 1.0\n",
    "    preds[preds <= 0.0] = 0.0\n",
    "    error = y - preds\n",
    "    W += lr * X.T @ error\n",
    "    \n",
    "for _ in range(10):\n",
    "    forward(X, W, y)\n",
    "    \n",
    "bias = W[2, 0]\n",
    "m = -bias/W[1, 0]\n",
    "k = m/(bias/W[0, 0])\n",
    "xx = np.linspace(0, 1.5, 100)\n",
    "yy = k*xx + m\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], color='tab:blue')\n",
    "plt.plot(xx, yy, color='tab:orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd4b9ed",
   "metadata": {},
   "source": [
    "## Multi-layer perceptrons\n",
    "Introduce more layers of perceptrons in the model and you get what is called a multi-layered perceptron (MLP). Here, we have layers that do not directly interact with the input $x$ nor the output $y$ and is referred to as the latent space. The introduction of a latent space greatly increases the models capabilities of fine-tuning the perceptron weights. Furthermore, if the perceptrons does not create any cycles and a perceptron in layer $h_i$ interacts with all other perceptrons of the next layer $h_{i+1}$ then it is referred to as a feedfoorward neural network. Images of MLPs are often what you see if you google 'AI' or 'machine learning', but most of them visually explain quite well what is actually going on, as can be seen below.\n",
    "\n",
    "![multi-layer perceptron](../images/mlp.jpg)\n",
    "\n",
    "### Updating the weights\n",
    "Updating the weights of a MLP is not as straightforward as for the single-layer counterpart. Now, since each neuronal output of an intermediate layer depends on the preceeding output we get inherent dependencies in our model. The way to update the weights is performed by means of the *backpropagation through time* (BPTT) algorithm. Starting at the output, we calculate the gradient of the loss with respect to all perceptron weights one layer at a time backwards. This is done while multiplying the output (gradient) of of the previous layer with the current layers gradient.\n",
    "\n",
    ">$\\large \\Delta w_{i} = \\eta\\frac{\\partial L}{\\partial w_{i}}\\cdot \\hat{y}_{j}$\n",
    "\n",
    "Since we iteratively are *multiplying* the gradients of each layer, if the gradient is smaller than 1 continually then the resulting gradient at the first layer will be extremely small, often close to 0. This is what is known as the *vanishing gradient* problem. Similarly can happen for large gradients, where it instead explodes throughout the layers. This is called *exploding gradient*. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
